{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Metrics obtained from an organizer of the Challenge\n",
    "# https://github.com/plamere/RecsysChallengeTools/blob/master/metrics.py\n",
    "from metrics import r_precision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpd.slice.0-999.json\n",
      "mpd.slice.1000-1999.json\n",
      "mpd.slice.10000-10999.json\n",
      "mpd.slice.100000-100999.json\n",
      "mpd.slice.101000-101999.json\n",
      "mpd.slice.102000-102999.json\n",
      "mpd.slice.103000-103999.json\n",
      "mpd.slice.104000-104999.json\n",
      "mpd.slice.105000-105999.json\n",
      "mpd.slice.106000-106999.json\n",
      "mpd.slice.107000-107999.json\n",
      "mpd.slice.108000-108999.json\n",
      "mpd.slice.109000-109999.json\n",
      "mpd.slice.11000-11999.json\n",
      "mpd.slice.110000-110999.json\n",
      "mpd.slice.111000-111999.json\n",
      "mpd.slice.112000-112999.json\n",
      "mpd.slice.113000-113999.json\n",
      "mpd.slice.114000-114999.json\n",
      "mpd.slice.115000-115999.json\n",
      "mpd.slice.116000-116999.json\n",
      "mpd.slice.117000-117999.json\n",
      "mpd.slice.118000-118999.json\n",
      "mpd.slice.119000-119999.json\n",
      "mpd.slice.12000-12999.json\n",
      "mpd.slice.120000-120999.json\n",
      "mpd.slice.121000-121999.json\n",
      "mpd.slice.122000-122999.json\n",
      "mpd.slice.123000-123999.json\n",
      "mpd.slice.124000-124999.json\n",
      "mpd.slice.125000-125999.json\n"
     ]
    }
   ],
   "source": [
    "playlists = list()\n",
    "tracks = dict()\n",
    "map_pl = list()\n",
    "\n",
    "max_files_for_quick_processing = 100\n",
    "\n",
    "\n",
    "def process_track(track):\n",
    "    key = track['track_uri']\n",
    "    if not key in tracks:\n",
    "        tk = dict()\n",
    "        tk['track_artist_name'] = track['artist_name']\n",
    "        tk['track_artist_uri'] = track['artist_uri']\n",
    "        tk['track_name'] = track['track_name']\n",
    "        tk['track_album_uri'] = track['album_uri']\n",
    "        tk['track_duration_ms'] = track['duration_ms']\n",
    "        tk['track_album_name'] = track['album_name']\n",
    "        tracks[track['track_uri']] = tk\n",
    "    return key\n",
    "\n",
    "\n",
    "def process_playlist(playlist):\n",
    "    pl = dict()\n",
    "    pl['playlist_name'] = playlist['name']\n",
    "    pl['playlist_collaborative'] = playlist['collaborative']\n",
    "    pl['playlist_pid'] = playlist['pid']\n",
    "    pl['playlist_modified_at'] = playlist['modified_at']\n",
    "    pl['playlist_num_albums'] = playlist['num_albums']\n",
    "    pl['playlist_num_tracks'] = playlist['num_tracks']\n",
    "    pl['playlist_num_followers'] = playlist['num_followers']\n",
    "    pl['playlist_num_edits'] = playlist['num_edits']\n",
    "    pl['playlist_duration_ms'] = playlist['duration_ms']\n",
    "    pl['playlist_num_artists'] = playlist['num_artists']\n",
    "    if 'description' in playlist:\n",
    "        pl['playlist_description'] = playlist['description']\n",
    "    else:\n",
    "        pl['playlist_description'] = ''\n",
    "    trks = list()\n",
    "    for track in playlist['tracks']:\n",
    "        map_pl.append([playlist['pid'], track['track_uri']])\n",
    "        trks.append(track['track_uri'])\n",
    "        process_track(track)\n",
    "    return pl\n",
    "\n",
    "def process_mpd(path):\n",
    "    count = 0\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in sorted(filenames):\n",
    "        print(filename)\n",
    "        if filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
    "            fullpath = os.sep.join((path, filename))\n",
    "            f = open(fullpath)\n",
    "            js = f.read()\n",
    "            f.close()\n",
    "            slice = json.loads(js)\n",
    "            for playlist in slice['playlists']:\n",
    "                playlists.append(process_playlist(playlist))\n",
    "            count += 1\n",
    "            if quick and count > max_files_for_quick_processing:\n",
    "                break\n",
    "\n",
    "quick = True\n",
    "process_mpd('mpd.v1/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(playlists))\n",
    "print(len(tracks))\n",
    "print(len(map_pl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_df = pd.DataFrame(playlists)\n",
    "playlist_df.head()\n",
    "# print(playlist_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_df = pd.DataFrame.from_dict(tracks, orient='index')\n",
    "tracks_df.head()\n",
    "# print(tracks_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_map_df = pd.DataFrame(map_pl, columns=['playlist_pid', 'track_uri'])\n",
    "playlist_map_df.head()\n",
    "#print(playlist_map_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set strings to lower case remove all non alphabetic characters and stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "ignored_words = ['music',\n",
    "'songs',\n",
    "'playlist',\n",
    "'good',\n",
    "'jams',\n",
    "'mix',\n",
    "'lit',\n",
    "'best',\n",
    "'stuff',\n",
    "'quot',\n",
    "'like',\n",
    "'one',\n",
    "'amp',\n",
    "'get',\n",
    "'make',\n",
    "'new',\n",
    "'know',\n",
    "'really',\n",
    "'back',\n",
    "'day',\n",
    "'days',\n",
    "'little',\n",
    "'things',\n",
    "'great',\n",
    "'everything',\n",
    "'jamz',\n",
    "'tunes',\n",
    "'artist',\n",
    "'song',\n",
    "'top',\n",
    "'listen',\n",
    "'favorite',\n",
    "'bops',\n",
    "'description',\n",
    "'top',\n",
    "'ever',\n",
    "'mostly',\n",
    "'enjoy',\n",
    "'bunch',\n",
    "'track',\n",
    "'tracks',\n",
    "'collection',\n",
    "'need',\n",
    "'every',\n",
    "'favorites',\n",
    "'may',\n",
    "'got',\n",
    "'right',\n",
    "'let',\n",
    "'better',\n",
    "'made'\n",
    "]\n",
    "\n",
    "def word_cleanup(df_col):\n",
    "    df_col = df_col.apply(lambda x: x.lower())\n",
    "    df_col = df_col.str.replace('[^a-z]+', ' ')\n",
    "    df_col = df_col.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    df_col = df_col.apply(lambda x: ' '.join([word for word in x.split() if word not in (ignored_words)]))\n",
    "    df_col = df_col.str.replace(r'\\b\\w{1,2}\\b', '').str.replace(r'\\s+', ' ')\n",
    "    return df_col\n",
    "\n",
    "playlist_df.playlist_description = word_cleanup(playlist_df.playlist_description)\n",
    "playlist_df.playlist_name = word_cleanup(playlist_df.playlist_name)\n",
    "\n",
    "#playlist_df.playlist_name = playlist_df.playlist_name.apply(lambda x: x.lower())\n",
    "#playlist_df.playlist_name = playlist_df.playlist_name.str.replace('[^a-z]+', ' ')\n",
    "#playlist_df.playlist_name = playlist_df.playlist_name.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def generate_wordcloud(text): # optionally add: stopwords=STOPWORDS and change the arg below\n",
    "    wordcloud = WordCloud(font_path='/Library/Fonts/Verdana.ttf',\n",
    "                          relative_scaling = 0.1,\n",
    "                          ).generate(text)\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "generate_wordcloud(' '.join(playlist_df.playlist_name))\n",
    "\n",
    "generate_wordcloud(' '.join(playlist_df.playlist_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "cv_description = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", stop_words=None, ngram_range=(1,1), analyzer='word')\n",
    "dt_mat_description = cv_description.fit_transform(playlist_df.playlist_description)\n",
    "playlist_df['playlist_description_frequency'] = list(dt_mat_description.toarray())\n",
    "\n",
    "cv_name = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", stop_words=None, ngram_range=(1,1), analyzer='word')\n",
    "dt_mat_name = cv_name.fit_transform(playlist_df.playlist_name)\n",
    "playlist_df['playlist_name_frequency'] = list(dt_mat_name.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install yellowbrick\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "visualizer = FreqDistVisualizer(features = cv_name.get_feature_names())\n",
    "visualizer.fit(dt_mat_name)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import FreqDistVisualizer\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "visualizer = FreqDistVisualizer(features = cv_description.get_feature_names())\n",
    "visualizer.fit(dt_mat_description)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_mat_description = tfidf_transformer.fit_transform(dt_mat_description)\n",
    "playlist_df['playlist_description_tfidf_score']=list(tfidf_mat_description.toarray())\n",
    "\n",
    "tfidf_mat_name = tfidf_transformer.fit_transform(dt_mat_name)\n",
    "playlist_df['playlist_name_tfidf_score']=list(tfidf_mat_name.toarray())\n",
    "playlist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_name_frequency = pd.DataFrame(dt_mat_name.todense(), index=playlist_df.index, columns=cv_name.get_feature_names())\n",
    "bigrams_name_frequency['playlist_pid'] = playlist_df.playlist_pid\n",
    "print(bigrams_name_frequency)\n",
    "\n",
    "bigrams_desc_frequency = pd.DataFrame(tfidf_mat_description.todense(), index=playlist_df.index, columns=cv_description.get_feature_names())\n",
    "bigrams_desc_frequency['playlist_pid'] = playlist_df.playlist_pid\n",
    "print(bigrams_desc_frequency)\n",
    "\n",
    "bigrams_name_tfidf_score = pd.DataFrame(dt_mat_name.todense(), index=playlist_df.index, columns=cv_name.get_feature_names())\n",
    "bigrams_name_tfidf_score['playlist_pid'] = playlist_df.playlist_pid\n",
    "print(bigrams_name_tfidf_score)\n",
    "\n",
    "bigrams_desc_tfidf_score = pd.DataFrame(tfidf_mat_description.todense(), index=playlist_df.index, columns=cv_description.get_feature_names())\n",
    "bigrams_desc_tfidf_score['playlist_pid'] = playlist_df.playlist_pid\n",
    "print(bigrams_desc_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    pd.merge(\n",
    "        tracks_df, playlist_map_df, left_index=True, right_on='track_uri'),\n",
    "    playlist_df,\n",
    "    on='playlist_pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Negative Samples\n",
    "negative_samples = pd.DataFrame([])\n",
    "for pid, df in tqdm(merged.groupby([\"playlist_pid\"])):\n",
    "    negative_tracks = tracks_df.drop(df.track_uri).sample(df.shape[0])\n",
    "    negative_playlist_tracks = pd.concat(\n",
    "        [\n",
    "            df.drop(list(tracks_df.columns) + ['track_uri'], axis=1).reset_index(\n",
    "                drop=True), negative_tracks.reset_index()\n",
    "        ],\n",
    "        axis=1)\n",
    "    negative_playlist_tracks.rename(columns={'index': 'track_uri'}, inplace=True)\n",
    "    negative_samples = negative_samples.append(negative_playlist_tracks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide labels\n",
    "negative_samples['match'] = 0\n",
    "merged['match'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = merged.append(negative_samples[merged.columns]).sort_values(by=['playlist_pid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the features in the list below\n",
    "features = [\n",
    "    'playlist_duration_ms', 'playlist_num_albums', 'playlist_num_artists',\n",
    "    'playlist_num_edits', 'playlist_num_followers', 'playlist_num_tracks',\n",
    "    'playlist_pid', 'track_duration_ms'\n",
    "]\n",
    "data_x = dataset[features]\n",
    "data_y = dataset.match\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_x,\n",
    "    data_y,\n",
    "    test_size=0.1,\n",
    "    stratify=dataset.playlist_pid,\n",
    "    random_state=42,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been split, which is great. However, in order to better simulate real world situations, we need to build a test set with a lot more negative samples than there are positive samples. In the split above, there are equal number of positive and negative samples, so it'll be easy for the algorithm to perform. Below we build a more robust test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_refined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()\n",
    "# y_test[y_test == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[X_test.head().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate metric only on test set using the positive samples\n",
    "#     a. Obtain unique playlists in the test set\n",
    "#     b. For each playlist, obtain the positve songs.\n",
    "#     c. Set both `target` and `prediction` as lists of the positive songs\n",
    "#     d. For each playlist, calculate the r_precision. r_precision comes from the metrics.py file in the repo.\n",
    "\n",
    "# 2. Train the X_train on the classifier. Obtain predictions for X_test\n",
    "\n",
    "# 3. Repeat 1 above, now setting `predictions` as the list of songs \"ranked by probability\".\n",
    "\n",
    "# 4. Calculate the r_precision metric based on your predicitons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_precisions = []\n",
    "for pid, df in tqdm(X_test.groupby([\"playlist_pid\"])):\n",
    "    probs = y_test.loc[df.index]  # change y_test to the output probs from clf\n",
    "    targets = dataset.loc[probs[probs == 1].index].track_uri\n",
    "    predictions = dataset.loc[probs[probs > 0.5].index].track_uri.unique()\n",
    "    if len(targets) > 0:\n",
    "        r_precisions.append(r_precision(targets, predictions))\n",
    "np.mean(r_precisions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = (X_train-X_train.mean())/X_train.std()\n",
    "X_test_norm = (X_test-X_test.mean())/X_test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf = lr_clf.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prob = lr_clf.predict_proba(X_test)\n",
    "y_prob = pd.DataFrame(lr_clf.predict_proba(X_test_norm), index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_prob.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_precisions = []\n",
    "for pid, df in tqdm(X_test.groupby([\"playlist_pid\"])):\n",
    "    labels = y_test.loc[df.index]  # change y_test to the output probs from clf\n",
    "    preds = y_pred.loc[df.index]\n",
    "    targets = dataset.loc[labels[labels == 1].index].track_uri\n",
    "    predictions = dataset.loc[preds[preds == 1].index].track_uri.unique()\n",
    "    if len(targets) > 0:\n",
    "        r_precisions.append(r_precision(targets, predictions))\n",
    "np.mean(r_precisions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_refined = pd.DataFrame([])\n",
    "r_precisions = []\n",
    "pbar = tqdm(X_test.groupby(['playlist_pid']))\n",
    "for pid, df in pbar:\n",
    "    labels = y_test.loc[df.index]\n",
    "    targets = dataset.loc[labels.index].track_duration_ms\n",
    "    positive_targets = dataset.loc[labels[labels == 1].index].index\n",
    "    # trained_idx = set(y_train[y_train == 1].index) - set(df.index)\n",
    "    # Obtain all tracks from dataset which are not in the playlist under consideration\n",
    "    #     negative_tracks = dataset.loc[X_train[(X_train.playlist_pid != pid)]\n",
    "    #                                   .index].track_duration_ms\n",
    "    negative_tracks = dataset.loc[X_test[(X_test.playlist_pid != pid)]\n",
    "                                  .index].track_duration_ms\n",
    "    # trained_tracks = X_train.loc[y_train.loc[y_train[y_train==1].index].index].track_uri\n",
    "    new_df = df.drop('track_duration_ms', axis=1)\n",
    "    new_test = negative_tracks.append(targets)\n",
    "    new_df = pd.concat([new_df.head(1)] * len(new_test))\n",
    "    test_playlist_tracks = pd.concat(\n",
    "        [new_df.reset_index(drop=True), new_test.reset_index(drop=True)],\n",
    "        axis=1).set_index(new_test.index)\n",
    "    # from IPython.core.debugger import set_trace; set_trace()\n",
    "    test_playlist_tracks = (test_playlist_tracks-test_playlist_tracks.mean())/(test_playlist_tracks.std()+1e-8)\n",
    "    X_test_refined = X_test_refined.append(test_playlist_tracks)\n",
    "    y_prob = pd.DataFrame(\n",
    "        lr_clf.predict_proba(test_playlist_tracks),\n",
    "        index=test_playlist_tracks.index)\n",
    "    y_prob = y_prob.sort_values(by=[1], ascending=False)\n",
    "    if len(positive_targets) > 0:\n",
    "        r_precisions.append(r_precision(positive_targets, y_prob.index))\n",
    "    pbar.set_description(\"{}\".format(np.mean(r_precisions)))\n",
    "    # from IPython.core.debugger import set_trace; set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(r_precisions, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(r_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Do playlist and track duration interact to influence whether a song should belong to a playlist `(match=1)` or whether a song should not belong to a playlist `(match=0)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset[dataset.match==1].playlist_duration_ms/dataset[dataset.match==1].playlist_num_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dataset[dataset.match == 1].playlist_duration_ms /\n",
    "            dataset[dataset.match == 1].playlist_num_tracks,\n",
    "            dataset[dataset.match == 1].track_duration_ms)\n",
    "plt.scatter(dataset[dataset.match == 0].playlist_duration_ms /\n",
    "            dataset[dataset.match == 0].playlist_num_tracks,\n",
    "            dataset[dataset.match == 0].track_duration_ms)\n",
    "plt.xlabel('Average Track Duration')\n",
    "plt.ylabel('Track Duration')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trend which might not be that strong to notice is that as the average track duration increases for a playlist, the propensity to see a track of long duration increases. However, this isn't the case with negative sample playlists, where in playlist of short length very long tracks can still be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dataset[dataset.match==1].playlist_duration_ms)\n",
    "plt.hist(dataset[dataset.match==0].playlist_duration_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
